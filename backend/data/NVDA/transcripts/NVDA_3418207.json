{
  "id": "NVDA_3418207",
  "participant": [
    {
      "name": "Debraj Sinha",
      "description": "executive",
      "role": "executive"
    },
    {
      "name": "Adam Ryason",
      "description": "executive",
      "role": "executive"
    },
    {
      "name": "Louise Huang",
      "description": "executive",
      "role": "executive"
    }
  ],
  "quarter": 0,
  "symbol": "NVDA",
  "time": "2025-02-27 04:01:00",
  "title": "Special Call - NVIDIA Corporation",
  "transcript": [
    {
      "name": "Debraj Sinha",
      "speech": [
        "Hi, everyone. Thank you for joining us today. I'm Debraj Sinha. I'm part of the Metropolis team here in NVIDIA. ",
        "Today, we have some exciting topics to talk about. We'll be talking about our NVIDIA AI blueprint for video search and summarization. This AI blueprint simplifies the creation of interactive visual AI agents that can analyze massive volumes of video data and understand activities using technologies like generative AI, visual language models and large language models. ",
        "Before we begin, I would like to cover a few housekeeping items. More information can be found in the top navigation panel. [Operator Instructions] A copy of today's slide deck and additional materials are available in the resource section. We encourage you to download any resources that you may find useful. If you encounter any technical issues today, please let us know in the Q&A box, and we'll help you troubleshoot. ",
        "So the idea is to build these insightful, accurate visual AI agents that can be deployed throughout our factories, warehouses, retail stores, airports, enabling operations team to make decisions much more better and faster from richer insights through more natural interactions. Today, we'll be also covering the developer journey on how they leverage this blueprint to build these AI agents. And we'll be talking about some of the improvements that we have made with this AI blueprint. For example, instead of going through the videos manually, we are seeing over 200x speedup using these AI agents to get insightful, critical insights from the video data. ",
        "We're also making it really easy for developers to try this AI blueprint. We are bringing a one-click deployment, which is called launchables, for you to just quickly try out these blueprint. We're also releasing GPU resources, compute resources, where you can deploy these AI agents on a single GPU. So for doing that, we are investing heavily on 2 types of technologies. ",
        "The first one is physical AI. Physical AI enables autonomous machines to perceive, understand and perform complex actions in the real world. The idea here is that Metropolis has evolved into a more robotics platform. It's not about the robots that move around in our spaces, it's about seeing things moving around in our spaces, having a more situational awareness of what are things going on in our spaces. So for doing that, we have over 1.5 billion cameras that are deployed around the world. There are millions of factories, hundred thousands of warehouses. So you want to understand at a given moment of time what is happening in our spaces. ",
        "So physical AI is enhancing the functionality and safety of large indoor spaces, like factories and warehouses, where daily activities involve a steady traffic of people, vehicles and robots. Using these fixed cameras, we can understand what is happening in our spaces, and leveraging advanced computer vision models, we can enhance operational efficiency by tracking multiple entities and activities within the spaces. ",
        "And the other technology that we are working on is called agentic AI. These systems ingest vast amounts of data from multiple data sources and, leveraging third-party applications, can independently analyze these data and provide critical insights. So these agents are powered by different AI models. And through natural language interaction through prompts, we can assign a job to it, and then it can provide us critical insights. So one of the critical components of these agents is the vision language model. ",
        "Vision language models enable us to interact with them through natural language prompts where we assign a job. And most of you are already familiar with vision language models. They are multimodal models that are capable of understanding and processing text, images and videos. They have the zero-shot learning capability where instead of reading the models every time you need to detect a new class of objects, these models are trained on large data sets. So it has the capability to detect a lot of objects. And another thing is that given its multimodal understanding, it can provide really good reasoning. You provide the images and videos, it can give really detailed answers about what are going on in those videos. Another critical thing about these vision language models are the enhanced retrieval capabilities. You can interact with this model to a natural language prompt where you can assign them a task, ask them questions, like what is going on in the video or the images, are there any hazards, are people present in the room, they can give you a really intelligent answer. ",
        "So let me show you some examples here. The input can be from anywhere. It can be from images, videos, live streaming cameras, even archived data. We pass it through a vision language model. And I'm just showing you 2 examples here. So I'm really asking  really complex questions, like for the first image, describe any safety hazards that are present or not and what is being done to fix that. So it gives you really intelligent answers, like, okay, there is a fire involved and the firetruck and the firemen are trying to control the flames. And similarly, for warehouse, we can build these AI agents using these vision language models where we can ask them really complex questions and they can give us some really insightful answers. ",
        "So we have these vision language models on our build.nvidia.com site, where you can try them out. You can upload your own videos or images, and you can ask questions to it, and you can get some really insightful answers. ",
        "So the idea here is to have this interactive, insightful visual AI agents deployed through our factories, warehouses, retail stores, airports, traffic intersections, helping enable operations teams to make better decisions much more faster with richer insights. ",
        "So imagine going through these videos. So we have millions of factories, thousands of warehouses. Going through these videos manually will take a lot of time. So we have this AI agent, which can automatically go through petabytes of data, hundreds of hours, thousands of hours of videos and provide us the insightful, critical insights that can help us to increase productivity, reduce waste, optimize our processes. That's why we built this AI blueprint, which is basically a recipe to build these AI agents much more faster, which can provide us critical insights of what is happening in the video. It can summarize. It can also provide Q&A for the user to ask questions and get some really cool answers. ",
        "So let me show you some examples right now. So these are some AI agents that are built using this AI blueprint. I'll be showing you a wide number of examples from different verticals. So for example, here, imagine you're a sports agent, and we want to provide a report of our CEO Jensen's baseball pitch. And it gives a really good answer. ",
        "You're a traffic agent, you want to analyze what is happening in your streets, in your cities. It seems like there has been [ an accident that is causing a traffic buildup ]. ",
        "You're a safety agent, you want to make sure your workers are following the right safety protocols. Here, we see a worker on a windmill, is this person harnessed correctly and is following the right safety protocols. This can be very useful for manufacturing as well. ",
        "You're a manufacturing agent, you want to make sure the workers are following the right standard operating procedure. So it looks like the third fan was not installed, and this can be notified early on in the production cycle so that we can reduce waste and optimize the processes. ",
        "Imagine you're a warehouse agent. You want to make sure our aisles are clear. There are no obstacles. You can ask questions in real time and get really cool insights back and you can act accordingly. It can also help you to optimize your operations. So there is a pallet on the floor. You want to understand which is the right space where you can put this pallet. It seems like the region 2 is the right space for this, and it is closely located. You want to find out if there is any unauthorized incident. So it seems like an unauthorized person just walking around in the warehouse. ",
        "Or you're a facilities agent, you want to see if there is any nuance activities like tailgating. So the idea here is that with this blueprint, we can create industrial-grade visual AI agents at scale that can enable us to find the critical insights out of those videos. ",
        "So let's see what is there in a visual AI agent. So this is just the pipeline of an agent. It can take inputs from images, videos, streaming videos. And then the user can assign a task to it using some natural language prompts and it can get summaries of hundreds of hours of videos, it can do question and answering and it can also perform alerts. But to build this AI agent, there is a heavy lift in the development process. It takes models like vision language models, large language models, computer vision and tracking. It has a lot of services in it as well, like doing media management and video ingestion, database management. All of this take a lot of development time and it can even take developers years to build it. So that's why with this AI blueprint, we have simplified the process here for anyone to build this AI agent. ",
        "AI blueprint is basically a reference application. Developers can use it as is or build on top of it with more capabilities. This blueprint is right now powered by NIMs. NIMs are basically NVIDIA inference micro services. These are containerized models, which are highly optimized to run on GPUs. So we have VLM NIMs, CV NIMs, LLM NIMs. And we're also providing over 20 different services doing database management, media management, packaging all this into this blueprint. So this blueprint is basically a recipe to build industrial-grade digital AI agents. And one of the improvements that we are seeing is that instead of going through the videos manually, we are seeing over 200x faster doing it with AI agents. ",
        "This AI agent provides a lot of flexibility to our developers. It is VLM and LLM agnostic. You don't have to use the NIMs. You're welcome to use third-party LLMs and VLMs as well. And also, we provide a lot of deployment options as well. You can deploy these agents in the cloud, in the on-prem, giving you more flexibility for your security needs so that the data never leaves your organization. ",
        "So now I would like to switch gears. I gave you an overview of this blueprint, what it does. So let's understand what is there in this blueprint. As you can see, it's a really complex architecture diagram. It has over hundreds of libraries, different NVIDIA technologies all brought together to build this AI blueprint. ",
        "So let me hand it over to Adam Ryason, who is our product manager for this blueprint, to talk more about what is actually done on this blueprint. Over to you, Adam."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Thank you, Debraj. What we have here is our architecture diagram or our reference workflow for this AI blueprint for video search and summarization. It's primarily broken up into 2 different areas: an ingestion pipeline and a retrieval pipeline. Each of these individual sections are broken down into smaller components that can be plugged and played or utilized how you see fit to your product use case or specific to a customer use case. ",
        "On the ingestion pipeline side, we have a deep stream, open source pipeline that can be customized to your specific use cases. We take in stored video as well as live streamed video, perform video chunking or breaking down that video into smaller pieces and then perform decoding and preprocessing on that video prior to doing frame selection. This ingestion pipeline, this data processing pipeline, can be augmented with other types of pipelines, including audio, computer vision or any other type of sensor data that you might want to include when feeding into your VLM. ",
        "Going into this data ingestion pipeline a little bit more and what's taking place. We take a video, let's say, we have a 60-minute video, and we chunk that into smaller pieces that the VLM would be able to understand in more detail. What we end up doing is we are able to take these individual chunks, let's say, a 10-second chunk, and pass these chunks into the VLM so that we can parallel process these over multiple GPUs and make this process extremely scalable. Users are able to tune the chunk duration based on the number of frames per second they would want to pass into the VLM. They can also allow for chunk overlap so that any events that might be happening at a chunk duration interface can also be properly captured. ",
        "There are a lot of aspects to this that can be customized, and we've seen different use cases perform better with various chunk sizes. For instance, if you have very fast events, let's say you're watching a sports match or you're watching traffic, you might want to look at events that are happening at less than 1 frame per second, maybe 0.5 frame per second or 0.25 frame per second. In order to do that, you would have a smaller chunk size, where if you're looking at maybe a smart spaces area that might not see a lot of activity or it's tracking longer activities, you can create a more efficient pipeline by increasing your chunk size and not having to process as many videos into your VLMs. ",
        "Now the next step, once these videos are preprocessed and chunked, they're then passed into the VLM with any accompanying metadata that might have come from additional CV metadata or audio metadata in order to gain useful insights from this VLM. The point of the VLM is to create a natural language description of what's taking place in that clip. So for instance, if you have a 10-second clip of a traffic area, you want to be able to capture maybe what cars are coming in and out of that intersection, how are they moving, what direction they're going in. And we're doing this on a per chunk basis. So you'll have a 10-second description or what we call a dense caption that is then processed and then brought together all underneath our RAG pipeline. ",
        "So on the retrieval side, we have 2 pipelines: a vector RAG and a graph RAG. The vector RAG portion right now is primarily used for creating summarizations. Once the dense captions are created from the ingestion pipeline, along with any metadata from audio or CV, this information is then stored in a vector database that can be retrieved later to things like summarization as well as Q&A. ",
        "The graph RAG pipeline has additional features to it that allow for better relationship information to be retained from the data that's ingested. So what this means is if we have people in a scene, if we have cars in a scene or people interacting with cars, we're able to identify those relationships that are happening specific to user IDs and retrieve that information from a graph database. Now either both of these RAG systems or either one of them can be utilized for all of the retrieval-based aspects. ",
        "So depending on your use case and the compute that you have available, you could utilize both or either one of them. And these are interfaced through an LLM. The third-party LLM pretty much right now, we have multiple NIM LLMs that work well with VSS. This includes Llama 3.1 70B is primarily what we use now, but you could also utilize it with Llama 3.2, Llama 3.3; DeepSeek R1 is one that we currently support. This allows for more reasoning to take place inside the retrieval and understanding the information that's being obtained. And you can also connect any types of third-party LLMs or fine-tuned LLMs that you may have already specific to your industry. ",
        "Now all of this information, all of this pipeline is primarily used for users to do summarization, alerts and Q&A. And on top of that, you can customize how you want to have your user outputs so that you could utilize this information or these databases for generating your own responses. You don't have to take this blueprint as is. You can take it and you can customize it. If you want to just use the injection pipeline, for instance, or if you just want to use the retrieval pipeline, you can enable and disable different parts of the helm chart in order to tailor this blueprint to your needs. ",
        "So over the past year, as we've been developing this, we've been able to see a speedup of around 100x for some of our graph Q&A as well as almost up to and exceeding 200x for summarization generation. Now to go into details on how these two differ is that in our end-to-end summarization, if we're just utilizing vector RAG, we are able to just -- we're pretty much just ingesting into the vector database and retrieving from that to create the summarization. Now generating a graph database does require more compute and therefore, the cap on what that speedup is limited, as seen here. ",
        "What these tests are showing is, on an 8x H100, where we're using a Llama 3.1 70B model for the LLM, and we're using a VILA-1.5 34B model for the VLM. So to give a little more description on this and a little more detail, for instance, if we had a 60-minute video, we would be able to ingest it and create a summary within around 30 seconds. And this is pretty much using the parallel processing that's on our GPUs. And then if we wanted to be able to ingest a 60-minute video and, let's say, create a chatbot that could interact with what's happening there, let's say, you have a TV show or you have a movie or maybe a keynote, and you want to question the agent what's happening in that media, you would be able to do that about after a minute after ingesting into VSS. ",
        "Now these benchmarks that we're seeing here are improvements that -- we've seen about improvements from the last 3 months around 20%. And we expect between now and over time, from now in the next 3 to 4 months as well, that we're going to see continuous improvements, so that we're going to have speedups even higher than what we currently have. And what speedups mean as well is that you can also reduce the compute that's required. So that will go down over time as we optimize the architecture and improve models that are being utilized. ",
        "So to go into more details on the benefits, as Debraj and I have highlighted up to this point, the first one is this accelerated time to market. Instead of you having to dedicate a large number of resources and a large developer team to put in together the architecture for the VLM, the databases, the LLM and the RAG, we're providing this prebuilt workflow that serves as a great starting point. ",
        "Second, we've been seeing that this is extremely cost effective at scale. Compared to any other option, this is the lowest total cost of operation. We have an extremely efficient data ingestion and processing. In fact, it's state-of-the-art when utilizing any GPU. We see an extremely high throughput with the VLMs and the LLM NIMs because they're so tightly interconnected. And in addition to that, you're not paying per inference when you're utilizing this pipeline. Once you have the hardware, you're able to inference as much as you want without having to think about the individual cost per inference. ",
        "Third is the ability for the extremely high accuracy and customizability of this blueprint. We are able to provide a great out-of-the-box experience for developers. This includes the VLMs and LLMs that are already trained on petabytes and petabytes of data, billions of images and videos that have already been trained. However, you have the ability to further fine-tune these models for your specific use cases and your specific products in order to increase the accuracy. ",
        "And then last is the ability for data privacy. This blueprint can be downloaded and it could be stored on your cloud. It could be stored on-prem. And none of the data that you are collecting as an organization ever has to leave your premise. You can do pretty much fine-tuning and deployment all on your local hardware without having to ever go to a third party. ",
        "Next, I'd like to introduce Louise. She is an AI model and fine-tuning product manager as part of Metropolis."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Louise Huang",
      "speech": [
        "Hello, everyone. Welcome to the webinar today, and thank you, Adam. So today, I'm going to introduce you the fine-tuning micro services for our VLMs. ",
        "As Adam and Debraj has mentioned, we are working very hard to provide you the entire blueprint that can provide zero-shot performances. But we also understand you might be needing some customization on top of that. So this is what I'm going to introduce today that we have been providing these micro services for the VLMs that NVIDIA has released, for example, the VILA models and also the NVILA models. ",
        "I checked the Q&A box that some of you had noticed that the link to the early access is not working. We're working on bringing it back to life today. Okay. Let's step into the details. ",
        "So we understand that in many cases, you might want to detect a very specific object or events or you are working on projects that needs a lot of domain technology or domain knowledge. So in those cases, we would like to introduce the fine-tuning capability of VLMs. And here is the complete end-to-end flow of how to prepare your data and what we provide. ",
        "So once you get your image and videos, you will need to work on a little bit of the data curation. There are many options to do so. For example, you can do human annotation to kind of split the videos based on action, for example, in SOP use cases and/or you can use LLM to help you annotate these videos, human and LLM annotations. So once you have that image or videos prepared with the annotation, we can go to the next fine-tuning step, which is provided in our micro services today. ",
        "So we provide multiple methods of fine-tuning capabilities depending on what you are looking for. For example, if you need a lot of feature extractions or you need a lot of specific events or object detection, you may want to do a full fine-tuning on the visual transformers. If you need a lot of question and answering or summarization improvement, we also provide the fine-tuning method on LLMs. So we provide 2 methods: one is the LoRa, the more parameter efficient fine-tuning; and also you can do a full fine-tuning on the LLM as well, depending on your compute need. After you fine-tune the model, you can go through an evaluation process on the accuracy improvement before you deploy it to your cloud or local infrastructures for your inference. ",
        "Let me walk through 2 of the examples that we have here. The first one we have here is we fine-tuned the VLM on top of a YouCookII data set. This is quite a popular research data set provided by University of Michigan. So before we fine-tune, you can see the caption, the summarization it gave is very, very, general. It says a man is cooking food in the pan and then covers it with a lid. However, in the actual video, you can see the camera movement. And after fine-tuning, our VLM begins to kind of pick up all these camera movement by saying it begins with the close-up. And also, it can notice the details in the images as well. It can notice like maybe it is cooking pasta and possibly some vegetable or meat and also notice the environment of the setup, it is kitchen setting, the person is not cooking in a dorm or he's not clicking in a random place. So these are a very straightforward visualization of what before and after, how much more information the fine-tuning model can capture. ",
        "Next, I would also like to walk you through -- sorry, I also had a little bit trouble of flipping the slide. Okay. Here we go. So the next example I would like to show is our partner, Pegatron. So Pegatron has been using human operators to identify the process on the assembly line. But they see a lot of inconsistency as well as human errors in the process. So we are preparing helping them deploy a camera system that can identify the SOPs and missed actions with a VLM. In this solution, we tried with our VILA 3B model, and then we were able to get a lot of accuracy improvement. Before, none of the missed actions were detected. But after we fine-tuned with only 8 videos, it can detect 98% of the missed actions or whether the SOP is correct. ",
        "I also want to mention that for a specific event like SOP, which has a very 1, 2, 3, clear steps, we will be able to only fine-tune with, let's say, 8 videos or 10 or 100. But if you are looking to, let's say, fine-tune the model towards a more generic environment, for example, you're detecting a warehouse event or you're detecting a smart space, you might need to use a lot more, maybe thousands of videos. ",
        "So that concludes what I'm going to introduce to you today. I'll hand it back to Debraj. Thank you."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Perfect. Okay. Thank you, Louise. Thank you, Adam. Yes, so with this blueprint, it provides you an end-to-end pipeline to build these industrial-grade visual AI agents at scale. So starting from just taking camera streams from your videos and building this agent together, getting summaries, you can also improve your accuracy, as Louise mentioned, like fine-tuning your VLMs. So let's talk about some real-world examples where we are seeing our partners using VLMs and NIMs to build these AI agents. ",
        "Siemens, one of the world's largest manufacturers, has built an industrial copilot to assist engineers with equipment maintenance task and even do error handling and do performance optimization. This industrial copilot is a generative AI-powered assistant that provides real-time answers to facilitate rapid decision-making and reduce machine downtime. So Siemens has been working on this copilot and have been able to show increase in productivity by 30% in their factories. ",
        "So another example is Deephow. They are leveraging vision language models and large language models to convert video demonstrations into an interactive and engaging training guides with AI summaries. And this is being used by Stanley Black & Decker to train their onboarding new employees, and they have been able to -- so they are using this AI assistant and have been able to cut their onboarding costs by half. ",
        "So I want to show you more real-world examples. These partners are around the world who are leveraging the blueprint to build these video analytics AI agents at scale. Imagine these agents are consuming hundreds and hundreds of hours of video data, taking input from thousands of cameras. We have been working with the top GSIs of the world, ISVs around the world. ",
        "So let's go through some examples. So we're working with Deloitte. Deloitte is building smart city applications, these AI agents, where they can be deployed into cities, which takes input from thousands of cameras, giving us critical insights about what is happening in the cities. ASOCS is optimizing factory operations. At a given time, workers can ask questions, get more information about the situational awareness of things like are there forklifts, AGVs, drones, getting more insights about what is going in the factories. ",
        "Centific is improving fan experience for live events. So this is an example of a soccer event where fans can ask question, when will the tickets be available, can you show me some soccer highlights. So just Q&A-based AI assistant, where it can provide you much more information. We are seeing from some companies like ITMAX and Linker Vision, where they are creating detailed reports of different incidents in the cities that city officials can leverage and improve on the response time. And SoftServe is optimizing their warehouses, manufacturing facilities with this AI assistant powered by the blueprint where workers can ask questions and get really insightful answers back. ",
        "So I wanted to show you some examples of a wide range of different verticals here around the world who are leveraging the blueprint to build these AI agents to optimize their processes and improve their worker safety. So now, as Adam mentioned, we have made tremendous improvement in our blueprint, and we have a new GA release that is coming up very soon. So I will hand it over to him. He will go over these new key features that are available and the improvements that we are making. ",
        "Over to you, Adam."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Thanks, Debraj. So for those of you who might be already working with VSS, you probably have experienced some of these features. But we want to provide you all the features that will be coming out in early April that will be part of our GA release. ",
        "First is a single GPU deployment. We've heard a lot of feedback where the amount of compute needed to set up VSS was hard. And we wanted to -- and it was fairly high, right? So we wanted to provide options that allow for users, whether you're small developers or you're a company who wants to try out a POC, we wanted to give you the ability to deploy this on single GPUs, sub-48 gigabyte GPUs, and we'll be working on those to provide that this April. ",
        "In addition to that, as far as features go, we are going to be fully providing RTSP short and live multi-streaming. This includes connecting multiple cameras and streaming all those cameras to VSS as well as taking in small 1-minute, 2-minute clips and parallel processing those in a way that allows you to detect key events. So in fact, this feature was released today. It was released as an early access feature. And so it's something that you can actually start testing now. ",
        "Burst mode video ingestion, this is pretty much the first mode that we came out with. This allows you for either long video or short video, a serial ingestion processing. So let's say you have a 30-minute TV show or you have a 30-minute video that you want to understand what is happening or create a report for, you can ingest that video and process it to generate a report or do Q&A on. ",
        "And one of the biggest features that we receive a lot of requests for is audio transcription. This will actually allow for the audio of a video to be utilized inside of the retrieval process. And so the way we do this is, within our deep stream pipeline, we split the audio and the video, and we perform a speech to text on that video, and we store that information in the databases along with that visual information. So you'll be able to say, when was this stated; or you can say, give me more details about when the speaker talked about a specific feature. And so not only will your retriever be utilizing the visual information, it will also be able to use audio information and connect those to retrieve timestamps or retrieve clips of that time. ",
        "Additionally, we're working heavily with different cloud providers to provide one-click deployments. This includes AWS, Azure, GCP as well as Brev launchables. ",
        "And then lastly, another highly sought-after feature, especially from companies that have existing CV models that they want to utilize inside of their agents, is the ability to incorporate custom CV models. So we will have a video ingestion pipeline that has computer vision. This will help you be able to detect or perform CV detection prior to feeding that into the VLM. So you'll be able to detect -- you'll be able to do a CV tracker. Let's say, you want to detect cars or you want to be able to count cars or the number of people that are going in, that CV metadata is going to be passed into both your VLM as well as passed into your database so that it can be referenced to when getting, generating the dense captions as well as when performing RAG. ",
        "So how can you get started with VSS today? Pretty much you can go on and apply for early access. And as you have been able to do for about -- EA has been available for 3 months now. So that process is still the same. You go on, you sign up for EA. You'll have the available helm chart as well as Docker Compose files that you can pull from. ",
        "But now, in addition to that, we've also introduced a very easy method for you to try VSS, and this is through Brev launchables. Brev is a cloud platform that allows you to easily deploy instances that have Jupyter notebooks associated with them. So we provide a one-click deployment for VSS on Brev launchables. Depending on the instance sizes, they start off around $14 an hour. That's the smallest instance that we are running VSS on right now is 8 L40Ss. However, if you want to really push the limit on what the performance is and you want to increase that to H100s or H200s, you can do so by starting up different instances. ",
        "So we provide an easy process for doing that today. Pretty much when you go and you click on that Brev launchable, you'll have the software pre-configured. It will pretty much show you the price for launching that, and it'll give you a preview of that notebook. ",
        "So just to demonstrate how easy it is, I'll walk through kind of the process that takes place here. So as a user, once you've signed up for EA and you have your associated API key, you'll be able to go to the notebook. Click start. The notebook starts spinning up. Pretty much it creates the instance. It sets up the software needed. And then you can enter in the notebook and then pretty much start running VSS by first utilizing the various NIMs that are inside of it. These launchables utilize a Docker Compose to set up. So you'll have to -- once you have EA access, you'll pretty much go obtain your API key, and this will be the only part of the process that you have to manually enter anything in. And you'll generate your API key. This allows you to get access to the containers as well as the NIMs that can be locally deployed. ",
        "So once the API is entered, you'll spin up the NIMs. Pretty much this launches the Llama NIM, the reranker NIM and the embedding NIM, as well as the VLM that is being utilized right now, which is VILA 1.5. This is downloading each of these NIM containers and deploying it locally on this 8 XL40 (sic) [ 8 L40S ] instance. This would pretty much be almost identical to if you had your own bare metal instance and you were running it on your own machine. So we wanted to replicate what that procedure would be like in this cloud-based instance. ",
        "Once all those NIMs are spun up and running, this takes a few minutes, probably around 5 to 10 minutes to download a 70 billion parameter model and then spin that up and start it running. You then will be able to deploy VSS. And VSS will pretty much download, it will download the container, it will download the model for the VLM and then generate the TRT-LLM engine for that model. This process does take a little bit longer. Right now, we're seeing it probably takes around 25 to 35 minutes to download that model, spin up the TRT-LLM engine, prior to you being able to use it. However, we are working on containerizing a lot of these aspects so that we can reduce the time to getting this launched even further. ",
        "Now once VSS is up and running, you'll be able to access it through forwarding through this user interface. You have some test demo videos there as well as some prompts that you can start interacting with and then you can summarize, you can perform Q&A on. And on top of that, once you've kind of acquainted yourself with how prompting works, how the responses look, you can start uploading your own video. You can upload your own clips. You can connect your own streams. You can test out VSS pretty much in all of its different features directly through this launchable interface. ",
        "So as I mentioned, it does take around $14 an hour to start depending on the instance size. And to help everyone get started, we want to provide you credits today to get started with this launchable. We're providing $40 of credits to users that are looking to get started right now. Pretty much you can sign up for EA access. You will go to the EA landing page, the developer landing page. Once you've been approved and you signed in, make sure you have your API key for that EA access, and then you'll be able to click on the launchable link. The credits will be added to your account and you can start testing out VSS immediately. ",
        "Now the next process in that, once you've tested out VSS in this demo environment, you want to start thinking about how you want to build with VSS. And this includes utilizing any types of public CSPs. So moving this deployment or utilizing the helm chart deployment or the Docker Compose deployment on AWS or Azure. Let's say you want to allocate DGX Cloud or you want to start allocating hardware for your own small-scale or medium-scale POCs. This is pretty much the next step in downloading and deploying locally to integrate with your own existing products. ",
        "And then last, when you're ready, once you've tested out your POC and you're ready to take this thing into production, we pretty much provide you with optimized deployment through these NVIDIA NIMs. These are all under NVAIE licensing. So you'll be able to take this, scale it to the sizes that you need and then deploy into production. ",
        "So I'll pass it back now to Debraj, and he'll cover some of the GTC aspects that we have coming up soon."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Perfect. Thank you, Adam. Yes, thanks for going over the developer journey of how to get quickly get started with this blueprint, build these AI agents. ",
        "So just a quick reminder, our biggest AI event is right around the corner. GTC is happening at San Jose on March 17 to 21. One of our most awaited events, our CEO Jensen Huang's keynote speech is also there. And you can listen to him virtually as well. So all the links are provided. We have amazing sessions, engineers, business leaders talking about how AI is transforming our world. So check out the link. ",
        "And I just want to post some of our important sessions focused on computer vision and building these digital AI agents. We have some feature talks from our NVIDIA experts. The one in green are focused talks. We have some workshops as well where we give hands-on training to build these AI agents. So do check them out. We have some really cool partner talks from partners coming from all over the world, talking about how they're leveraging VLMs and the blueprint to build these AI agents. ",
        "So that was the presentation part. Thank you so much for listening. So let's go into the Q&A. Give me a couple of minutes to just go over the questions."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Let's get started with some questions. I'm seeing a lot of questions on the blueprint. So Adam, a couple of questions to you. Can you elaborate more on the roles of the vector and graph RAG components in the blueprint?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Yes. So we primarily utilize the vector RAG for any type of unstructured data. This is data that we take in. It's primarily used for the summarization aspects. It's great for being able to compile the database into a structured format. And then the graph database and graph RAG component are primarily used for identifying relationships between different entities inside of that database. So it's very helpful for doing any types of temporal or spatial localization when asking questions."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Got it. And in your presentation, you mentioned that the AI agents that are built using this blueprint can be run on a single GPU. Can you talk more about it and what is available now?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Yes, yes. So we're actually working towards the single GPU deployment. The way that it works right now is NIMs are designed in a way to maximize their throughput on a given GPU. So when we end up putting multiple NIMs on a single GPU, they all want to maximize their usage on that GPU. Now we're developing constraints in order to allow for multiple NIMs to run on a GPU. And once this has been developed for NIMs that are specific to this blueprint, we'll be able to do that. And so it's kind of a -- there are a few challenges, some of them being memory-based, others being design-based. So we're working towards being able to first make single GPU deployments available for H100, L40S, A6000. And in the near future, we're going to have RTX 5090s as well as digits for single GPU deployments."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Got it. And what are the different VLMs and LLMs that we can use with this blueprint?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Well, I think it's a fairly open-ended choice to make when it comes to the LLMs and VLMs. We pretty much allow for any LLM or VLM that is open AI API compatible. So what that means is most LLMs that are being developed and hosted on Hugging Face all have this API spec. So as far as LLMs go, this includes Llama 3.1. This includes DeepSeek R1, any of the Llama models. You can also interchange any types of open source or closed source, third-party models directly into the blueprint and utilize those as well."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Got it. And this is part of the video ingestion. Is the chunk size fixed before the ingestion process begin? And can you change it while you're processing the videos?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Right. So no, as of today, we have only fixed chunk sizes. However, there is research right now that's taking place that's looking at dynamic chunking. So in cases where this would be very useful is if you have something that's possibly not taking place on the screen very often or if you have any other types of detections that might be telling your ingestion pipeline to reduce the -- to either increase or decrease the number of chunks. But as of today, it has to be fixed for a given stream or video."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "I'm seeing some fine-tuning questions. So Louise, over to you. How do I fine-tune the vision language models for my specific use cases? And how much data is needed?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Louise Huang",
      "speech": [
        "Yes. So for fine-tuning the VLM for specific needs, actually, you need to kind of analyze on what you're trying to solve. If it's a very specific set of steps, for example, like the SOP example we showed, you might need less videos. So like I mentioned earlier in the slides, around 10 or 20, sometimes 100 for better, like better accuracy and robustness. But if you're trying to fine-tune for a generic scene, like warehouse or retail stores, then in those cases, we are seeing the need for thousands of videos to get a quite robust fine-tune model in those cases."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "And what kind of annotation is required for videos for VLM fine-tuning?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Louise Huang",
      "speech": [
        "Yes. So it depends on the customized use case as well. So if you have, let's say, a question-and-answer that you are interested in detecting, you will need to annotate that corresponding to the video. So in those cases, you need to use a little bit of the curator to help you kind of prepare the data sets for fine-tuning."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "And can I fine-tune the blueprint according to accurately detect timestamps in the video, like if you're looking for any specific events based on the prompt?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Louise Huang",
      "speech": [
        "Yes. We have work in progress of putting the timestamp, the temporal understanding in our VLM for fine-tuning. We are looking at around GTC time frame or around GTC time frame for another EA release as well."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "And can you elaborate on the hardware needed to fine-tune these VLMs?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Louise Huang",
      "speech": [
        "Yes. So if you're using the VILA model 1.5, the commercial version, the minimum for fine-tuning, we've seen 4 A100s. That's the very, very minimum. And for the upcoming NVILA model with high-res capability. Because of the high-res feature, we would require at least 8 H100s, so 1 node of GPUs."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "And can you provide some insights on fine-tuning models for  different industries, let's say, health care or even farming?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Louise Huang",
      "speech": [
        "Yes. We're definitely exploring these use cases as well. We have some internal experiment going on with health care. We also see promising results from the fine-tuned model. And also, while we focus on the warehouses as well as the retails for Metropolis and NVIDIA, but it can be definitely propagated to, let's say, farming, as mentioned as well."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "And you mentioned that the fine-tuning micro service is available. So how can one get access to it?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Louise Huang",
      "speech": [
        "So we have a link to apply for. If you search for the slides, we have early access link. Please go ahead and submit the application form. Then your application will be received."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Perfect. Thanks, Louise. And the link is also mentioned in our resources list. So yes, definitely check it out, apply for it. I have more blueprint questions. So over to you, Adam. How many video streams can the blueprint concurrently monitor?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Yes. So at the end of the day, it really depends on a handful of things. One is what GPU you're looking at. Two is whether you want to have that go into a vector database or go into a graph database or you want to go into both of them. So we've done tests right now with H100s primarily for multi-streaming. The way that we're seeing it is, when monitoring, we're able to do ingestion for pretty much having up to around 30 streams per GPU when doing just summarization. So in this case, if we're just ingesting those videos and we are pretty much storing those into a vector database that can be created for summaries, that's pretty much the maximum that we've seen. When we are looking graph RAG, we're seeing around 10 to 12 streams per GPU when maximizing the number of streams."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Got it. And is there any support on running these AI agents built from the blueprint on edge devices like the Jetson?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Yes. So we're working towards Jetson support now. This should be available either at GA, which is in April, or it will be available shortly after that. So we are developing right now the support ARM platforms with the blueprint."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "And there's a question about cost structure. If you need to run these AI agents using the blueprint for an hour, how much does it cost?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Yes. So that's another thing that kind of like it depends on what you want to run it on and what speed you're looking at. I think what's top of mind right now for myself is, if we were to run this on a cloud instance, let's say, on Brev, we would probably -- it would cost around $14 per hour at a minimum, and that would be for 8 L40s. Now if you wanted to increase the performance and you wanted to run this on 8 H100s instead and say you want to maximize the throughput of your videos or maximize the number of streams, it costs between $30 to $50 an hour. And these prices pretty much are pulled from the various cloud providers like Google and AWS."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "And can this agent be configured to raise alerts over time or even in the future?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Yes, yes. So right now, alerting is a feature that's already implemented both from inside of ingested short video, streaming video and multi-stream live video."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "And do we have any workshop sessions on this? So yes, one I would  to add is that for GTC, we do have 3 workshops based on visual AI agents. If you're attending the event, do check it out. And also, we are working on a DLI course on building AI agents using this blueprint. So that will be coming up very soon. So we'll be notifying you.",
        "Okay. So how does this blueprint perform when processing live videos?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Sorry, you broke up a little bit. Can you repeat the question?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Yes. How does this blueprint perform when processing live videos?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "When you say how does it perform, I guess, do you..."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "In terms of performance, how does it work when it feeds from RTSP camera streams?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Got it. So the goal for processing RTSP streams is, let's say, you still would define a chunk size. And the way that it works is once you're pretty much streaming the frames to the blueprint as they're coming in. However, let's say, you have a chunk size of 10 seconds, after 10 seconds, it will look at all the chunks that were fed into it at that point and then provide you the dense captions for that. Now the goal for VSS, to be able to completely analyze that chunk and pass it to the database before the next chunk needs to be sent in, to prevent any types of latency issues. So for instance, you want to maximize the number of chunks that can be processed within 10 seconds before the next 10 seconds come in, and so that's heavily -- that's pretty much how the maximum throughput is calculated for multi-streaming."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "This is a latency question. What is the expected latency if the solution -- you're taking video feeds from thousands of cameras, let's say, 1,000 cameras."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Yes. So that's pretty much -- just to build upon my last answer because they're directly tied to one another, if you want to have responses, if you pretty much want to have dense captions generated every 10 seconds, the maximum number of cameras you can analyze is based on pretty much the number of chunks that you could process so that you don't exceed that 10-second ingestion time. So that's pretty much -- that's how we calculate the maximum number of streams that we can do in multi-streaming."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "And this is a question on the VLM that we use. Does the VLM have temporal understanding? Or is it only for -- has frame level understanding?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Yes. So there are a few things right now, pretty much there are. We do have a VLM that does have temporal understanding, and we do have VLMs that don't have temporal understanding. So if you're using a VLM, for instance, that doesn't have temporal understanding, VSS is still able to provide you chunk-level timestamps. So it will be able to identify maybe a range of time that something is taking place. However, if you're using one of our LITA-based, LITA, as in the temporal-based models, you are able to get timestamps for when events take place."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Got it. So I think we have time for one last question. Is there a way to split a large LLM or VLM model over multiple GPUs by learning this blueprint?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Yes. So you're able to -- so for instance, depending on -- there is a minimum size of a GPU that you would need. But you are able to deploy any -- you're already able to deploy these NIMs, LLMs and VLMs across multiple GPUs. So that's how we do it right now where we have multiple L40s that are hosting an LLM, multiple L40s that host the VLM, and you're able to scale and parallelize the process across those GPUs."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Debraj Sinha",
      "speech": [
        "Good. Thank you, Adam and Louise. Once again, thank you all for joining us for this amazing event. An on-demand version of this webcast will be available approximately 1 hour after this event ends and can be accessed using the same link. Thank you again for joining us today. Have a great day. Goodbye."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Adam Ryason",
      "speech": [
        "Thank you."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Louise Huang",
      "speech": [
        "Thank you."
      ],
      "session": "management_discussion"
    }
  ],
  "year": 0
}