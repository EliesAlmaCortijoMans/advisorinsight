{
  "id": "NVDA_3418206",
  "participant": [
    {
      "name": "Bharat Giddwani",
      "description": "executive",
      "role": "executive"
    }
  ],
  "quarter": 0,
  "symbol": "NVDA",
  "time": "2025-02-27 08:31:00",
  "title": "Special Call - NVIDIA Corporation",
  "transcript": [
    {
      "name": "Unknown Executive",
      "speech": [
        "Good afternoon, good morning, good evening, depends on which part of the region and were is joining us today. I welcome you all on NIM Microservices. This is David Arlo. I am the developer and Charter -- and today, webinar, you got a pretty much gelatin AI, understanding the latest trend and challenges in deploying the agent or -- and also, I would see this revenue as a game changer for you. And you would possibly understand in the speaker to how you can significantly improve the performance and efficiencies of your GenAI models and applications that you're building. Finally, I think there's going to be a lot of demos and earlier world use cases that the speaker is going to take you through. So hang in for their engaged with most of as some of it down goals for today's session, I would like to take your attention to the navigation bar on the right side of the console you would possibly would love to see who your speaker is usable explore profile and under the speaker bio. You would also have the auction of Q&A. So if you have any questions around the webinar beyond the webinar and [indiscernible]  our technology, feel free to ask your question there. There's also a feedback and survey tab. ",
        "We would love to have your feedback at the end of the free and go and share your experience there. Within the resources section, there are ultimate amount of resources in terms of pit resources in terms of the [indiscernible] stories, articles you have for Vanicrevices. Anything around the topic of the webinar, you will find there. So please go and explore the resources under that section. Within the Q&A at window, we have the solution architect team, presently to answer some of the questions. So during the wean, I don't hesitate to ask any of the questions for moderators, what just an in to give you the feedback or the youth answers for your questions. So engage with them on the Resources section under the Q&A section. ",
        "Beyond this, one of the other aspects I want to get started with this before I introduce Bharat. We have all got you engaged to the GLI promotion. And this webinar is going to give you an opportunity to go and explore the DLI courses. And the we'll talk more about that in regard for the QR cord and the [indiscernible] to sign up and gain access to the PLI post for you. With this, I welcome and introduce a speaker for the day. Bharat is a very seasoned senior architect within the Ended solution architect team at India. He has a lot of experience around -- and his deep and his expertise is now large language models plus my tie model as well as a -- and his competency, I would say, is more of engaging with the customers, providing them the end-to-end solutions and also optimizing GB performance to the effect of resource management and cost optimization strategies. ",
        "Bharat also comes with a larger background of MNOs and cloud architecture. Beyond this, he is available and later, he has a great fan following a mitna a paste to explore and join him on that part and [indiscernible] any questions you might have on Invista. With this, I open up the forum and hand over the session to Bharat, over to you. Thank you."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Bharat Giddwani",
      "speech": [
        "Thank you, Gavita, for the nice introduction. So -- let's get started with the session team. So we are going to talk about accelerate your AI development life cycle with NVIDIA micro services, NIM micro services. by NIM, we mean NVIDIA inference microservice. This is somewhat a new technology. We have recent -- we have released last year that helps you optimize your generative AI and conversational AI pipeline up to the mark on NVIDIA GPU hardware. Along with me, we have a few moderators, as Kavita mentioned, we would be able to answer your queries in the chat. Feel free to ask them during the webinar and at the end of the webinar, we'll also take some of the questions and discuss it live -- by going live as well. ",
        "So with that, we just talk about the agenda today. Today, initially, I'll talk about the basic introduction about Generative AI, like how it started and what all you can do now with NVIDIA. Moving forward, we'll talk about it moving forward, I'll talk about NIM micro services that explains about the basic overview to advanced architecture and then how you can deploy it across different platforms, be it in the cloud service providers, data centers or any work station. And moving forward, we'll talk about what kind of performance benefits you can get with NIMs and we'll also talk about recent development that we are doing with NIMS called us NIM Blueprint that helps you identify what all types of modern applications who can build the agent or flows you can build with NIM micro services. There are certain examples. We have enabled it. Feel free to test it out. It's all available on our website. We'll then share the resources where you can access all of them. ",
        "And finally, we'll talk about certain custom demos that we have built in India and also a case study with one of the customers that is being successful in using NIMs and other framework that allows them to build the entire ecosystem of [indiscernible] application. In between, I'll also take you to some of the experimentation like how easy it is to just download on NIM and run it. And then what kind of complex application you can build. So a few live demos as well will talk about here and see how it works. So with that, let me just start with the video. ",
        "[Presentation]"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Bharat Giddwani",
      "speech": [
        "So you have seen the idea about the NIMs like how it actually simplifies your entire workflow Chennai building. majorly will talk about the nuances in the later slides. But as you could have seen that this was one of our blueprint from the NIM agent blueprint suit that we have called as digital human that allowed you to create Altar, which involves multiple sub NIMs working in simultaneously in an accelerated shift mode like some speech to text, text to speech, audio to face, then on tour with large language models, vision language models and whatnot to make the entire pipeline look interactive and conversational. If anyone wants to create such piping, this could be one of the accessible NIMs available on the agent blueprint. Now moving forward, let's just give basic introduction about Generative AI. ",
        "It started with a GPT in like last 3 years back around when you might have noticed that about within just 2 months, people have started seeing the usage of AI has boomed, right? This was the one of the first application of that crossed 100 million users within just 2 months. But then people realize that this can be a possibility to make it into production can be used in various different industries. And then where from 2023 to 2024, people started experimenting with lots of different applications around AI and started building open source technologies like some models by Meta pipe tenants and whatnot Investia. And they produced equally competent models in the domain alongside with that, people also started creating the optimized frameworks to build and deploy these applications into production that could help achieve masses. And this year, all you could see there are lots of open source technologies available now to make it -- that are widely available to audiences. ",
        "But now, is it easy? The answer would be yes, the answer would be no. So to make it a reality, NVIDIA has created a lot of different suites of frameworks one of them is NIMs that helps you deploy generative application, be it in the domain of text to text, image to video, video to image or text to image, any domain or even in the domain of health care, we are enabling all these open source models into an accelerated form with various open source available packages like [ StensrTLLM, ] BLM, SD-LAN and whatnot to create their optimal version, but also in a much more easier way so that you could be able to post them in no time and also generate industry-specific endpoints that can help you create end applications much more easier. ",
        "So let's just understand more about them. First, as I just already explained, NIMS -- so there can be 2 ways you can deploy any application either through you can take a managed generative a service, which are highly accurate, lots of providers are providing them and also easy to use. Whereas the other way could be around like you deploy your own model from the open source and try to build the entire stack of optimization to serving to then writing industry-standard BPI endpoints so that the end data scientists would be able to create an application for the software engineer can create the final software stack. That leads to lots of time consumption. ",
        "So to make it much more easier we make it possible with NVIDIA NIMs. So I'll just take a quick overview of how it looks like. This is the base architecture of NVIDIA NIMs. It works on all the different kinds of platform providers, be it hyperscalers or local CSPs? Or you can make it work on even your own hardware, which consists of NVIDIA [indiscernible] But for the -- we always make sure that performance-wise, we always make sure from the performance base, you are able to achieve the right optimal results so that you can have the lowest possible BCO and best ROI. Alongside with that, we also make sure that you have the industry standard endpoints coming in with NIMs. Someone is mentioning that, do it use trite, do it use not in few of the cases, it does use and in few cases, it doesn't. We make sure wherever we are getting the best performance, we would be able to achieve that. And the most important part is, it is available in the form of containers. ",
        "So that it is portable and shareable to anyone, any other device and easily scalable as well, which also alongside just the containerization, we also make sure you're also able to get the right metrics for you to benchmark the numbers as well as for you to scale it further to the next level by providing the metric, system level metrics so that you can be able to auto-scale the outscale our containers and ports in your actual deployments. So to summarize, what is the difference between NVIDIA NIM and do it yourself way? NVIDIA NIM can help you reduce your time to market in just 5 minutes would be able to pull a container along with the optimized engine model that is available for most of the well-known models in the domain, if I talk about specific elements like Mistral, Quen, Metaslama, star order, et cetera, and some of the NVIDIA's own models are available. You would be easily able to pull and run them with ease. Alongside with that, we make sure that you get the industry standard API end points. It's not just in the domain of [indiscernible] . ",
        "If I talk about translation or speech to text, the AB and [indiscernible]  will look very much similar to what you are currently already using with the posted APs, Plus it supports -- it has support for additional features, which are crucial nowadays to build agent or close to build pipelines, which uses custom models. So you would be easily able to fine tune a adapter model like using [ LoRa ] and it is easily able to support hundreds of adapters depending upon the GPU memory. In your deployment case, you just have to do in a model that any of the open source method takes more framework or so. Once you have the adapter model trained, you can simply add it in the directory where NIM is downloaded, and it will automatically load it. And on the client side, you just have to add one more plant with a folder name where actually the model is applied and wouldn't be able to run your custom inference with LoRa. ",
        "We are trying to add more features along with that. You'll see in the coming releases. Apart from that, you can also find there are techniques like function calling, agentic calls are available within it, which helps you run the entire process of tool calling agentic workflows with ease and it's available in the documentation, you would be able to see if it's very much similar to what you're currently already using [indiscernible] API, the hosted providers [indiscernible] So this makes the entire process easy and available for go-to-market. So now as you already know, many store office stacks are being used in building such kind of application except [indiscernible] of TensorRT, VLLM, then the libraries for fast API and whatnot. And also the right version of Guda, Kurian Kublas, all these will also be required to make sure the entire process works well. And this maintenance is highly important encumbers to make sure we run it properly and all dependencies working well. ",
        "We do a lot of testing, and we also manage these as containers in one of our platform called NGC. So see, for every new release, we have everything updated and tested well before we publish it. So I'd like to take you to a page and also, I would suggest if you are already -- if you have not already tried it out, do try out tools, try that out to log in into the page and see how it looks like. So there is a page called as NGC. If you just simply search ngc.nvidia.com.  full form of NDCs NVIDIA GPU Cloud. It's not a cloud platform, but basically a platform where you will find all the cloud-related frameworks, which involves containers and the models, and the resources that we host for you to quickly run your test on your own environment or any other cloud service provider, be it hyperscalers or local.",
        "So if you just simply search NIM, so you can already see there are a lot of NIMs that are available on the screen. Initially, you would also be in the welcome guest mode, but I would suggest you to log in, if you want to pull any of the container from NGC, you have to login once, then there is a setup guide available within it. You have to follow that, generate an API that you be using  download the models. Once downloaded, you can easily run it on your own system without connecting to the Internet also. So there are more -- not just continues, also the hand charts are maintained. So if I just simply search container. Lama, I have to name it. So if I just simply take Lama test quarter 8 billion model, here you can see when I look and you will be able to see that. But when I login,you'll see all the different versions of it will be available. I'll just quickly do it. ",
        "So you can see we have released the 4 versions of the [indiscernible] Lama 3 container. So once you pull this container, this is the image name and once you pull it. And alongside with that, you will also be able to download the model's right profile based on your hardware -- system hardware that you had. So I'll cover this in detail in the next slide with me, just stop sharing and then go back to the mass light section. So when you -- so we will cover more about how basically we have divided the different engines, different optimized models for different hardware in the back end. Meanwhile, before moving forward. If you -- on your scheme, you might be seeing a poll. If you can answer this question, would be able to gather more information like how you are participating. So it would be great if you answer this question, and we'll talk more about like how it's architecture looks like in the next slides. ",
        "So we'll wait for 30 seconds. So it's a multiple choice question. You need to select 3 benefits of the NIM. So moving forward, let's see the answer. Most of you are correct. The first 3 are the right answers where you can deploy it anywhere on any hardware into marked time to market is low as well as the -- it provides that industry trade [indiscernible] to get started. Now moving forward, let's understand again the nuances what all things are involved within NIM. It's not just an optimized container optimized engine file. It is more than that. It involves the entire life cycle, be it if you have a system with drivers already stored you can use Kubernetes or the docker way to run a NIM? And on top of it, alongside with that, it provides to the industry standard API endpoints and the usage of all the necessary NVIDIA software stack, be it Tencate TLM, Tritan and Francesa depending upon the NIM that you are using, be it to text image to video or pitch domain of the model you are using. It uses the right framework site libraries and optimizes the model and run it on your environment. ",
        "Can run on one GPU to multi-GPU because it has the support of [indiscernible] , pipeline tells and so on available based on your hardware that you are using according Alongside with that, you can attach multi Lora adapters to it so that you can get different responses with -- from different users. They are using a different adapter altogether. And at the end, it provides through the feature of expecting all the metrics and locks, be it time to fasten end-to-end latency, request per second or the system level logs so that you can visualize them in [indiscernible] And this is the overall architecture where user will be just sending a request to the industry standard influence like where it can be in the usage of lying chain, Laminex, openirectly or [indiscernible] and then it will be going to a cloud-native environment where it is posted through either Kubernetes, Docker and Altametrics are coming in. ",
        "And in the back end, we have also optimized the preprocessing and post-processing steps on the GPU and try to make them parallel as much as possible. You'll see a few more updates in NVIDIA GTC with this -- for this architecture, where you'll see a lot of new improvements are coming in. I will share the link in the end. And to attend that and find out the latest updates and the latest features. But this is the overall architecture. It currently supports 2 backends. You'll see more backings are available for optimization steps for your large language model. And we also make sure that all the optimizations are already redone for the respective hardware and the optimizations are available with 2 engines, one is Tensa TLM. You just go and search and find out more information about it. You know it's one of the optimazation engine that helps you run like optimize our models in the terms of carry cacheing and then finalization techniques and then precision calibration and so on. ",
        "Alongside with that, we have BLL support as well available for [indiscernible] model deployments. And at the end, you would be easily able to run more chat completions and also you'd be able to see the metrics from the NIM -- so this is the overall overview. And I already talked about the LoRa concept that once you have the model fine-tuned with one of the techniques we other so on you would be easily able to deploy it with your current NIM deployment. And it all depends upon how many GPUs you are using, you can easily be able to scale accordingly and each user with different or adapter can be able to get the different results. One great important features we have added recently, if you are fine-tuned your own custom Lama model or open source any model, you would be easily able to optimize it [indiscernible] the steps are mentioned there in the documentation and you would be able to transfer the customized model as well into the NIM architecture with the just single click deployment.",
        "Finally, one more feature is tool calling and a pool calling and usage of Lama stack APIs within NIMs. So you would be able to do the function falling features, create agent take work flows and talk  the real type pages as well with the usage of NIM and without you writing any additional [indiscernible] This is how actually it identifies which system to -- which particular profile to use, whether it is sensor whether it should be FD precision, BF16-precision or FP16 precision. We have made sure for each GPU, we have all the types of profiles available. And based on the architecture and the customers need, they can select the type of profile from the same docker and command which pulls the container and run it. But you have flexibility to select it by default, it automatically selects the FP precision if you are using 100, [indiscernible] if you are using 800s or A series of the GPUs, it will pull the BS 16 format of the container. And that way, you would be able to get them and mostly, it was the throughput, high throughput profiles. ",
        "But if you have a requirement where you want lower latency, they are profiles available for the lower letter. So accordingly, you can test based on your use cases. Now I have one more question from the slides before we just discussed. If you can answer, again, there are 3 answers to it. Let me know. Okay. So now let's see how it looks like. Again, most of you are correct, the first, second and the fourth option is more suitable one, but it can be the third as well, but it's not just about chatbots. It's more than that. You can even work with NIMs in other generative applications. That's why I have not selected that as the answer. So let me again reshare my screen and now talk to about how actually you will find out in real world it works, right? So how actually it will be looking like. ",
        "Let me share it. Again, as I showed you, from NGC, you would be able to pull the container first and first, you need to set up your environment and set up side is present here. Go, feel free to explore that. You would find out mechanism to generate an API key so that you would be able to pull a container from NGC. And you have to just do docker login and also set up NGC CLI command once you have done that, you would be able to pull a of the data in your poker environment. So this is one of the systems I am just using to explain you how it looks like. So the simple -- the command is very much simple as well as similar to what you might be currently already using, simple docket and command with the mention of the number of GPUs as we are currently using a Lama 70 billion model which, in this case, requires 4 GPUs because we have created the profiles for TP with TPI enabled, which [indiscernible] you need to have at least 4 GPUs to make it work. ",
        "You need to mention the name of the profile. Once you go to the documentation page of them you are able to find the like how you can find out what all profiles are available for my GPU hardware for this particular NIM container. I show that as well in the documentation piece. On high level, you have to first mention the profile name. In this case, I'm using a BLLM,BF16 formated PP4 anti-1 profile. Then you have to mention your NGC API, which we just generated. If you go to the setup, you will find out the ways to generate it. And finally, where you want to store these engine trials and optimized preprocessing and post process modules entirely. Once you have provided the path you would be able to launch it. Generally, by default launches on 8,000 ports, so you can map it to any port as you want, and mention of your actual image name and that. ",
        "So I've already done that set up. You can see the logs. So it will look something like this. Takes some time, like hardly 30 seconds and then it would be able to deploy. So if you just see NVIDIA SMI. So as you can see, it uses Triton in the back end for the usage. So it has started initializing the model. Once you are pulling it for the first time, it will download the model as well. In my case, it has already downloaded. So download time is obviously something you need to take care of. For the first time, once you have downloaded, you can be easily able to scale. So we are now able to see that it has occupied the system memory and it has downloaded -- it has deployed the complete model. Now you would be able to differentiate it that is, you don't have to do any other kind of optimization because the engines that we have pull has already done that. ",
        "So I've created a simple stream for showing a demo. So if I just say high, currently just LLM is working. Onset speed is pretty good. The response time is well even for the model like of Lam billion. Now on top of it, if you want to do further. So I want to show further optimization with NIM. It's not just about [indiscernible] i, as I told you initially. It's all about other domains of the model as well we have enable. So I'll just show you a demo for a [indiscernible]  where I've already encapsulated certain documents. You can also, in this case, I have also added the feature for adding a few more documents. So I'll just add one random document, which is a scheme document from Indian government. There are multiple schemes available. So I've just taken this period. So you can see that this document is also not rightly actualized. It's -- we have to apply certain end of OCR or a certain kind of ways to build final tech. So that is being done within the itself in the back end. Again, for that as well, we are using our vision language model as a NIM for converting this images into the right text format. ",
        "So I built the index solely at the previous that history. And I'll start asking question. So you can see it has now -- it can now answer more, not just related to a basic hi, hello, what a normal LLM could do, here, it can be able to answer the questions related to the document. So if I just ask it related to, say, for example, something -- so it has generated the response from the document. And in this case, I'm using majorly 3 big NIM models. One is Lama 3.370 billion, which is LLM for the generation task. Before that, for embedding generation and then reranking task as well, I'm using 2 different NIM models available on NIM documentation piece. I'll show that where you can find all these information. One is LaMa3.2, 1 million model-based embedding model we have released pretty accurate as the total limit up to 80 to ingest and also supports dynamic embedding so that it's not -- you don't have to generate only the larger embedding size. Currently, I'm using [indiscernible] , you can reduce it to 512 or 1024 and so on to accelerate the performance further. ",
        "And the reanchor model is again based on 1 billion Lama model. We have released these all 3 models along with the vector database accelerated on is working and giving you this final research in an accelerated way. Along with that, you can add more types of NIMs be it in the domain of speech translation, extra speech and whatnot. That's just a quick example for the speech recognition as well. I think I need to enable -- so I'll show the demo at the end again for the speech to text part. Am I enabling that first? So this was an overview of the workflows and the demos. Now how and where you can find this documentation that I was talking about. We just have to search NIM documentations and NVIDIA. This is the way I search. You have to go to the NIM. There are multiple NIM subsections are available. The most important one, today, we are focusing on is large language model. Here, you'll find all the information that I'm talking about and how you can use tool calling, adapters, et cetera, et cetera. ",
        "Apart from that, if you are also interested to build something like ag is something called NIM or retriever. The NIMs available for embedding and reenters are available. We have support for 3-4 models with high accuracy. Alongside with that, there are models in the guardrails domain as well if you are applying that you on top at the end from [indiscernible]  to reduce the and content safety, you want to apply and then sale picked you can do this that need [indiscernible] Now moving back to 1 more page called us build nvidia.com. So there is one more page called is built on nvidia.com where the goal of this page is to explore and discover how NIM actually works and will it be suitable for my use case or not. So on this page, you'll be able to find that all these models that I'm talking about, we have already hosted in our environment for you to come and have a feel about it and explore how -- what all features would be available. ",
        "When you do it on yourself, on your own system from a cloud service provider or on your box stations, you can see in the reasoning section, all the elements are posted here starting with Deep Sea to Lamar and so on. And many other models are available. You just have to click Explore and we would be able to see all of them. In the domain of vision as well, some of the models are available for you to try it out and what. So just taking one example, if you open the Lama 3.37 billion, you can see a chat window on the left side, and you can see the response is quick. Alongside with that, I was always talking about the industry standard endpoint support, right? So it has all the types of response formats available, [indiscernible] with OpenAI can simply instead of using -- so in this case, instead of using integrated Apt.com, you can use our IP address and port combination, along with your API. Once you have pulled the model, you don't need to even use it. And the model that you have selected, -- is it 3.3? Is it or something else? ",
        "Then once you run it in this you would be able to do the inference with Sime client at completion or in completion normally. Apart from this, we also make sure that we have support for all the well-known frameworks that people are using for building their parent or a use cases. like supply chain, you'll find the support in LAMA Index, crude RTI and whatnot and even [indiscernible] so that you can -- you don't have to write your own wrapper and then run the application. Plus on top of it, if you don't want to work directly in Python despite turn and you want to use it in the shelf set or the simple call command, you can interact with the model with that as well. ",
        "So this is available for all the different domains as well for you to try out an ex payment. Feel free to go to the buildout nvidia.com page and try out these experiments. And then work accordingly and try to pull the necessary container on your own system. So let me go back to the slides. So I was always talking about like you can deploy the communities you can deploy with and chart as well. So we have a good resource guide available for you to try it out on almost all the hyperscalers and there are certain nuances you need to take care of, like what kind of PMI I need to create and how -- what kind of system I should be having cloud providers, so that I might be able to run it. So all the information related to that is available in the pickup pages of NIM deploy. I show that link as well on [indiscernible] but you would be able to find all -- pretty much all the information there. Plus, if you are doing -- if you have faced any issues, feel free to reach up to the to the NVIDIA forums where you would be able to get the responses done within 24 hours, like how do did on your own if you are facing it. ",
        "Apart from that, all this is working with something called a NEM operator. So it's another library on top of NIM for LLM, in the same documentation pad I showed you, there is NIM which walks on top of Kubernetes, create an automated life cycle for you to deploy these LMs or speech or any other domain models effectively with this operator. So it's like Kubernetes operator, but specifically targeted for the next to improve and easy the process. This is the overall architecture that helps you extract all the metrics. And then for the auto scale, the ports that you have deployed. It also has the features to save all the information in the cache and you would be able to accordingly run the complete life cycle. You don't even have to use [indiscernible] to pull the models, it all can be done with container waste Kubernetes. And you can just apply the seating secrets all the information and then it would be able to run the and then create a service out of it. And then you would be able to post it with your own -- you don't have to do it with the support of any additional open or [indiscernible] So another question like how can you access the [indiscernible] can answer. So there can be more than one answer again for this question as well. ",
        "Yes, almost everyone is correct that everywhere you can run the NIM, if you -- until unless you have NDP there, you'll be able to run them patiently. Now the performance highlights, quick performance highlights would be able to gain a lot of performance gain. So we have seen -- recently, we released on more block. You just go and check it out with the latest GPU that we have, sorry, with the latest GPU that we have released recently, B200 was able to achieve up to 25x performance with NIM on top of it. Same goes for the Deep seek model. Right now, again, you can see in this case, if you don't use the optimized version of the pipeline, you would not be able to get the best TCO for your end applications. It's always suggested to go with the optimal version of it. One of them is sure to optimize it through NIMS. Now again, it was just till now about the models. Now let's understand apart from the models individually, which is text only or visual on the or speech only. But as we can do, what else pipelines are available within NIMs? ",
        "So there are -- there is something called a NIMS blueprint that we have made it available. That is a reference example workflow for certain industry domains, be it health care or a conversational workflow domain, be it like calls and or so on, we have created certain blue brands alongside with that certain helpful developer blueprints, also we have created called is such as [indiscernible]  of them that helps you extract the documents, if it is imperious or any other format in well-structured way all images, or tables and text will be extracted properly with the suit of different NIM working simultaneous to each of them. Let's see how it looks like. So there are multiple blueprints available. One is I already showed you initially, which is digital human blueprint. That allows you to create [indiscernible] real looking at us at different models in place. When I talked about multi-DFdata extraction called as NV-intor you to extract the information from a period that can be used for creating a multi-model [indiscernible]  then blueprints in the domain of drug discovery and protein structure, omniverse, and other domains are also available for you to consume and run it effectively. This is the overall pipeline for digital human for customer service, which uses 6 to 7 different models, all working simultaneously. ",
        "You can use -- choose to use open source models or closed ones. We have enabled it for both. Then this is the period of extraction or called us [indiscernible] pipeline, which takes the period as input uses [ LOX ] model, which is a custom tuned model by NVIDIA to extract the images in charge separately. And then if it is charged then it will be sending to another vision language model like called as [indiscernible] by Google to identify the information from the chart and then use certain other models like cache and [indiscernible] to expect you more information that can be stored as text and that can be used for further processing and creating another version of the document that is much more rich in text and can be used for building any of that pipeline, which -- with the help of same embedding or reranked model that we were using. And alongside with that, other processing other images like some native images can be done with modest how much you went to vision models or so on. And you can expect out the final logic. So all basically a reference example, you can modify and create your own version of it and use it according. There are lots of customers already using this and have adopted the fetus. ",
        "So now final poll question for today, and then we'll move to some more demos and then fill accodingly. Where are the NIM and NIM blueprints available to try it? I just showed you one website where you can tie out the posted NIMs just check, let's see what you can answer. And just sharing if the answer is it is just a single choice answer. It doesn't have multiple responses. So hopefully, you'll get a better response for you to just without any hosting on your own environment, you can try out the page and get the feel [indiscernible] Okay. Let's see. Some of you have wrong answer. The answer should be buildout nvidia.com where I showed you the website where all the reasoning vision models, visual models all are available, but most of you are correct. So there you can go and explore how these models can work from the client end so that you would be easily able to run them as well into our environment when you want. And for that, you would need the access of NGC because the containers are present there for you to put.",
        "Apart from NIMs, we are also created a suit of frameworks, sort of libraries but different tasks for the entire ecosystem of generative applications. Starting with data preparation, we have something called this NimoCurator that helps you expect out the necessary data from the suit of Internet steel data that you might have accessed by applying deduplication, applying certain filtering techniques like some domain filtering, quality filtering or applying PII reduction, all these can be GPU accelerated and can be applied to your trillion scale tokens, and can expect out the necessary documents for your pretraining or fine-tuning task accordingly. So this is one ability we have available for you to try out and explore. If you're building something of your own, building a model of your own, this can be very much helpful for you to create the pipeline. Now on top of it, we have something called a Nemo, also called as Nemo Aligner,  Nemo customizer for you to pretrain and fine tune these models. ",
        "So I was talking about [indiscernible] with BMO -- sorry, LoRa fine-tuning with Nemo. So within this nemoframework, you have the techniques, all the necessary optimization was available like [indiscernible] and pipeline pales, context pedis, sequence prism, then apart from the Pellis technique like selective actuation checkpointing and so on are supported within this framework with just one conflict change, you would be able to tune the training process. with ease. And once you have the checkpoint available, we have the evaluation mechanism available with the open source data sets for you to find out whether you have improved it effectively or not. And then we already talked about Nemo [indiscernible] within that embedding and lead anchor models are present, and you can check out the documentation. And on top, is another framework called is NemoGarden that helps you apply [indiscernible] topic control and also helps you provide a proper flow to the pipeline, if you want to create something at chatbot or so on, you can restrict [indiscernible] in the models responses with the help of Nemo Gardes. ",
        "And finally, as we already discussed, you can deploy with NIMs. So let's see the screen share where all these presents will also share it with you. So you can simply go and search for Nemo -- although a better such would be name Nemo framework. If you saw nearer all the different links Complete documentation is available. You can see the release notes, or the user guide is present within this if you scroll down, you'll find Nemo Aligner, Nemo [indiscernible] or everything is available. All the bits are maintained separately. If you see the Nemours you'll find the information about how you can apply all the safety checks after -- during the deployment phase for the Nemo curatorAll the filtering techniques and expecting out the rich data you can do with Nemo Curator. So this all you can explore for building the entire life sector. Let's come back to the case study and the demo. So this one quick demo I have created for Indian languages so that you can -- you can feel like with the fine-tuning and the retaining techniques that are available with Nemo, you can create your own models in the domain of speech, textile speeds and LLM and then also deploy them easily with and get the good performance. So let's see how it looks like. ",
        "[Presentation] ",
        "[Foreign Language]"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Unknown Executive",
      "speech": [
        "So if you get the idea, basically, you could see the demo has the streaming speech to text, streaming text to speech as well as the ellumall working together. -- along with LLM also, there are embedding retriever and free ranking models were there for you to expect out the information from a vector data base and also reduce the final results. So this all was working fairly and effectively with ease so that you can also create something similar [indiscernible]  I was wanted to show. So for this particular demo, this is a case study you have to let me go back. So the next slide would be muted by default for you. If you want to -- for you to listen, you have to come on your screen. And on the left side of the screen, there is speaker option would be there. So just enable that and you would be able to listen that. This is the case study we just cracked for the and the South Asia work which we have done with Tech Mahindra. ",
        "So if you just go to the slide and on the left side, you will find out a speaker option just enable that for you to listen. So that's it with the case study, but now let's see where all you can find all these information about how to experience the NIMs, then how to extract out all the data. So there's one simple typo here. This is the build.com page where you can find and experience all the NIMs. Then to download it, you need to access the NGC hub. And then there is [ hugging taste ] page, where you'll find out all the models. In the previous video, you might have heard about in the Nemoto model. So these models, then there are models related to quality filtering domain filtering, then certain LLM models that we find to all are available on Hagen case NVIDIA go there and you would be able to find out those models that you can access and use it in your premises, if you want to work with them. ",
        "Now one more interesting and exciting thing we need to offer for all of you is that we are providing free BLA access to you. One of -- so these BLAs are charge from $30 to up to $90 generally. But if you have exited this workshop, you would be able to find out either of one of the BLA for free, use it. It's just you have to first log in into the page and find out the BLA or the workshop that interests you most. And choose one of that, and you would be able to log in at free of cost. If you have already registered with the [indiscernible] ID in this particular box shop. So I would wait for 30 seconds for you to log in into this page. You can select one of the workshop and you would be able to gain the access for free accordingly. So the success will be it for 30 seconds and then we'll share one more exciting information post this slide. And at the end, we will go through a discussion session on then one of my colleague about certain important NIM questions. ",
        "All the complete agenda is present in the respective course as well. So if you are interested in adverse machine learning or omniverse accordingly select that and log in. But with your e-mail ID, you can only select one of the costs, not more than that. Okay. So with the interest of time, I'll just move forward. Hope you all have taken the link. Otherwise, we will anyway share it with you. So now you can find out all the resources again on this page calls developer.nvidia.com, either just dive it out on the Google or you can again take the access from this QR code all the information related to start-ups and ISVs, partnerships that we have and the software stack from the hundreds of domains that you might have noticed just in previous slides are available in this page. ",
        "Now finally, NVIDIA GTC is coming on March 17 to 21. It is available both in person and virtually. With this link, you'll be able to register for 3 and as well as you would be able to gain lots of session insights that you would be finding out interesting later as well after GTC, if you don't have time during this period, if you register with this link, at the end of the GTC as well, you would be able to listen all the session that happened during that period. It also will cover lots of different advancements in the NIM itself that would help you even further optimize things or if we make it efficient. So feel free to log in with this link and further have exciting news enabled in your. We will share certain links after the session as well for you to get the registrations done for and we will see this particular session as well as the other sessions in GTC that you might have a [indiscernible] . ",
        "So I'll wait for 30 seconds for you to scan this goal. If you have any issue, let us on the chat and then we will move towards the Q&A session. Within GTC's page as well, you would be able to find we have made lots of filtering available like if you are targeting certain industry and certain domain. For example, you want to listen more towards the research topics you want to know more about the optimization topics you can accordingly filter the talk and add to your calendar so that you will get the notification as well in your -- during the session that as well, you would be able to see them, but would that link is different. We'll share it with you once you register here and after the [indiscernible] there. Any questions regarding this, let us know in the chat. Is this slide working for everyone? I can see some people are mentioning slide smart loading. Still, I'll just go back and come back to the slide for some of you, if you don't see it. You can refresh it at your side and then see maybe it will be visible. So one more minute, then we'll start with the Q&A, and then we'll end the session and we'll share the links to the registered e-mail ID and then you'll be able to gain the access for both CLI as well as the on-demand recording of the session."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Bharat Giddwani",
      "speech": [
        "Okay. Sounds good. Then let's start with the Q&A round now. So along with me. I have my colleague with [indiscernible]  would be discussing about the cushion answer that we have selected during this period. And let's talk more about that in detail."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Unknown Attendee",
      "speech": [
        "Can you hear me?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Bharat Giddwani",
      "speech": [
        "Yes."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Unknown Attendee",
      "speech": [
        "So a few of the things that based on what you were covering today, can you also sort of explain me the process of -- is it possible to integrate LoRa adapters with NIM framework  and if yes, firstly, can you just touch base on what exactly is the meaning of a lot adapter and how you can integrate that in certain framework?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Bharat Giddwani",
      "speech": [
        "Sure. So if I give a quick example, as, for example, you might be generally using these models with most -- mostly with the frameworks like land chain, Lama index and so, right? -- where you don't actually find out the way to use a custom tuned model in their API endpoints in there like [indiscernible] classes basically, right? So for example, with NIMs, we have -- if I talk about an call. We created something called a land AI endpoints that helps you import [indiscernible] and NVIDIA as the class and then that can be used for inference. ",
        "Now if you want to use native models, it is pretty much simple. You can just mention the hosted URL and the name of the model. But alongside that, if you want to use a specific adapter model, which you might have tuned with libraries like some unlogging base during libraries or nevoframework, you would get an adapter module -- if you're using LoRa, you might have noticed you will get another audio that you would be -- you would have to use it in the folder where you have downloaded the NIM container, containers model actually. There, if you just host this model alongside with the actual LLM, you would be able to use it with the -- you would be able to use that in the land chain or open air core call itself. We just have to add one more parameter to it that will be calling the particular directory where you have posted. So it's pretty much easy with NIMs to access from the client as well as to cost during the inference. ",
        "You don't even have to rerun the docker command for the hosting process. Once NIM is hosted, you just have to load it in the directory and then you have to restart the docker and that's it, to you would be able to see the model is presented in that and easily be able to import. So let me see if I find any client code also that would make you learn more. Meanwhile, you can answer the next -- we can talk about the next question, and I'll show that as well resharing the screen. ",
        "So with this, there can be one more question, which I would like to ask you. So there were things like questions like how basically these modules work and identify which hardware has the best suitable optimized version of the NIM profile so available. So if you can answer that question, like how different types of optimization techniques are available within NIMs that it uses from the back end sector [indiscernible] identifies which should be taken for the specific GPU or the hardware from the user."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Unknown Attendee",
      "speech": [
        "So basically, and I also see a few questions in the chart around similar grounds. Essentially, if I were to deploy any -- an application in production, right? This is an inference architecture for elements from our perspective, which we believe that this is the best practice, which you should be following for production scenarios. That is what our name is, right? Now inside the NIM, as engine when the core will be a model assume a lower model and this can be any model that can be a Hindi model as [indiscernible] -- now what we do is like based on the preview what from a hardware and authorization perspective, we work around with different backends. For example, there are open source tools like [indiscernible] -- these are different compilers of optimizing these LLM architectures, making them ready for infant deployments. ",
        "So what we do is since [indiscernible] I want to second what we will do is we'll run a lot of sweet search and we'll ensure that the specific current or that particular model or think of it like a component of the models are optimized for a specific hardware. So you will see our 100 profile, which we recommend running on [indiscernible] there will be 100 to recommend planning on [indiscernible]  There will be 4 like a enables architecture-based profile, it will be compatible with the [indiscernible] On GPUs, for example, is 200, right? There will be super profiles for those the lower grades like the previous generation, for example, NPA might not support FD, it doesn't support FDA precision, whereas the hopper generation supports -- so we'll ensure that the engine and the corresponding optimizations are how they are supposed to for one particular hardware, and that's what we -- in fact, that's what we sort of bundle when you download that particular end in what you were showing from the NDC. ",
        "So yes, on the similar ground, right, let's assume I have downloaded times and I want to deploy on H100 particular Lama model. I wanted to ask like if you covered around Kubernetes's hence -- is there any metric exportation also like, for example, if I am working with any enterprise, I myself and enterprise want to deploy the NIM and want to export certain metrics, is that supposed in NIM for impairment?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Bharat Giddwani",
      "speech": [
        "It's a very good question. So yes, we have a lot of different offerability features available within NIMs for you to explore. For example, let me show it by sharing the screen as well. So this is available within the documentation page itself, you can see. You can use these metrics, which will be coming on  of the port postspend that can be used in formative or gap on this page, you can see the information about from Sapna and visualization that you'll see. You'll find out the count latency values in all the was time to first open time to benchmark it. All these can be expected as well as can be used put charge, say, for your own customers or for your own use, you can identify the future, identify the usage. All these is available at the metrics endpoint. So it can be even expected with the [indiscernible] comments wherever the model is deployed by default, it is deployed on 8,000 port if you simply add even metrics, you will get insights for all of them. ",
        "And identify if there is any bulk during the deployment or what ways I can use to auto scale the board and that you can write it at NIM to work with. Apart from that, it can be used in -- like you can do the benchmark in 2 ways with NIMs. One, we have a suggested way of benchmarking tool available called [ GenAI hub ] that you can use to do a benchmarking. You can see on the right side, [indiscernible]  mentioned and the whole information about what actually request per second means and how it does the performance. Everything is visually available there. Apart from that, if you are viable using open source base or doing benchmarking like LLM, these also can be used and you would be able to benchmark their figures as well and accordingly defined during the Kubernetes deployment to have automatically scaling your pipelines. Yes, I think this is good for this. Let me know about something related to..."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Unknown Attendee",
      "speech": [
        "So one more thing, Bharat,if you could show the blueprints, different lupins, for example, the Ragland VLM Blueprint, and the element, if possible, could you share the links to share the screen?"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Bharat Giddwani",
      "speech": [
        "Sure, this is a good question. I missed to show it in the [indiscernible] , but it is again available there only. I showed the model individually, which is Lamar that time, but on top, if you can see, apart from models, we have blueprints also hosted here. So if you go here, you'll find a lot of different use cases or the reference examples available that are easily customizable for your own environment? You want to use some of our models, some of your own. Those things can be a certain -- for example, one pipeline like [indiscernible]  Ports. It's just a flood of media. And then it does all the incision around the entire engation pipeline to expect the necessary information. And then it runs an LLM to identify what can be sent to speaker on, what can be sent to a speaker to.",
        "And then accordingly, it will go to either elinLabs,-TTS or your own hosted details like either with NIM REVA or so on, you then would be able to get the final voice, which you'll be talking to like 2 speakers talking one example. And the entire architecture is also mentioned here. It's not just one thing that I call tent a list of prompts which will go to the list of different models in simultaneously. Obviously, you can make sure to use the one that you have and accordingly modify the pipeline. Then I talked about multi-model PDF extraction. So examples are available here like -- and it's -- the good part about the work flow is it can run at a [indiscernible] pay as well as in parallel. So you pick PDF pool, it would be able to expect the tax table and the image description in textual form, in both markdown as relation format for your fact to consume a bit efficient. Then there are others as well. So I'll just quickly show a quick demo if it's working sometimes, obviously, it's posted at our environment and multiples are using. It might not be working directly. ",
        "But you can see there are 2 ways of digital humans also available 3D of par, which the demos you also saw. And the 2D of par is available depending upon the [indiscernible] according the Vetas present. In this recording, you won't be able to listen the voice, but you can ask any question, you can allow to voice and ask any question to Aria. So this will come and accordingly answer this. You might not be able to listen it. But in your case, if you try it out at your end, it would be audible, but due to this platform limitation, sometimes you want to be able to listen it. But here, you will be able to find all the blueprint for you to experiment and then accordingly optimize. We are also partnering with lots of libraries and the companies like -- so [indiscernible] to create such agents."
      ],
      "session": "management_discussion"
    },
    {
      "name": "Unknown Attendee",
      "speech": [
        "I think that move to [indiscernible] and the"
      ],
      "session": "management_discussion"
    },
    {
      "name": "Bharat Giddwani",
      "speech": [
        "Thank you, everyone. And if you missed registering, I will  launched this slide again, if you can -- if you have not done it yet, just say, go to the link and try to do it now. I think we can close in after one minute. We will wait for one minute for everyone to log in. ",
        "So there are questions like, can we use it in windows NIMs like you can use it. There are ways to return like, but you have to use WSL in that case. So the resources that I have shared about both in the slides, about how to deploy it with cloud service providers. There as well, you'll find the information about if you're using WSL, just type VM or [indiscernible] ines, how to do it, that is available. And even in the documentation of the [indiscernible] is available. One more link I can share on the website. Let me show you that in the screen, it's although available mostly. I think it's not shared properly. Many of the -- many of you are asking like how to evaluate RAG and what's our best suggestion to create use cases. One blueprint area you can explore. Apart from that, you also maintain a lot of different industry specifics. A big example and value to it by creating a lot of different use cases. ",
        "You can go to this pick-up [indiscernible] examples by NVIDIA. They are within the Ag section and within community section, you'll find a lot of different examples are present plus the in drag, you'll also find a pace not just to do that, but also evaluate it and observe it. So the observability features and the evaluation features are explained in this notebooks and repository, you would be able to explore how effectively you can find out the entire process is going and accordingly optimized that. So the complete traceability is maintained here. Thank you. Yes, we can now close the session. Thank you, everyone, for attending the session."
      ],
      "session": "management_discussion"
    }
  ],
  "year": 0
}